[dmc_get_instance_info(1)]
# this is used in the 'Instance to Query:' dropdown list
# note that the role name is something like: indexer, search_head, cluster_master, license_master, deployment_server
# note: the reason we do "dedup serverName" is that, if `dmc_get_instance_info(*)`, then it's possible to get duplicate records since search_group is not a mv-fields
args = group
definition = inputlookup assets.csv | search search_group=$group$ | dedup serverName
iseval = 0

[dmc_get_machine_info]
# chose the first instance on that machine
# this is used in the 'Server to Query:' dropdown list
definition = `dmc_get_instance_info(*)` \
| stats first(host) as host first(serverName) as serverName by machine
iseval = 0

[dmc_get_instances_on_a_machine(1)]
args = machine
definition = `dmc_get_instance_info(*)` \
| where machine = "$machine$" \
| dedup serverName
iseval = 0

[dmc_get_groups]
definition = inputlookup assets.csv \
| fields search_group \
| dedup search_group \
| search search_group="dmc_*" \
| `dmc_set_group_label_and_sort(search_group)`
iseval = 0

[dmc_get_instance_roles]
# get roles of each instance
# the only difference between role and group is that, group includes custom group, which has prefix: dmc_customgroup_
# role group: dmc_group_(indexer, search_head, cluster_master, deployment_server, license_master)
# custom group: dmc_customgroup_somename
definition = inputlookup assets.csv \
| mvcombine search_group \
| eval role = search_group \
| mvexpand role \
| eval role = if(like(role, "dmc_group_%"), role, "") \
| eval role = replace(role, "^dmc_group_", "") \
| mvcombine role
iseval = 0

[dmc_set_group_label_and_sort(1)]
# produce well-formatted group name and sort groups: build-in group first, custom group second
args = search_group
definition = eval label = replace($search_group$, "^dmc_\w*group_", "") \
| eval label = case( \
    $search_group$ == "dmc_group_indexer", "Indexer", \
    $search_group$ == "dmc_group_search_head", "Search Head", \
    $search_group$ == "dmc_group_cluster_master", "Cluster Master", \
    $search_group$ == "dmc_group_license_master", "License Master", \
    $search_group$ == "dmc_group_deployment_server", "Deployment Server", \
    $search_group$ == "dmc_group_kv_store", "KV Store", \
    like($search_group$, "dmc_customgroup_%"), label."  (custom)", \
    1 == 1, label." (Unrecognized)") \
| eval order = case( \
    like($search_group$, "dmc_group_%"), 100, \
    like($search_group$, "dmc_customgroup_%"), 200, \
    1 == 1, "500") \
| sort order, $search_group$ \
| fields - order
iseval = 0

[dmc_collection_interval]
# this is for calculating runtime of searches, because the data.elapsed in _introspection index is logged every 10 seconds.
definition = collection_interval = 10
iseval = 0

[dmc_set_index_internal]
# in case some product has different name for _internal index
definition = index=_internal
iseval = 0

[dmc_set_index_introspection]
# in case some product has different name for _introspection index
definition = index=_introspection
iseval = 0

[dmc_set_index_audit]
# in case some product has different name for _audit index
definition = index=_audit
iseval = 0

[dmc_classify_processes]
# categorize processes into different classes.
definition = eval process_class = case(    \
    process=="splunk-optimize","index service",    \
    process=="sh" OR process=="ksh" OR process=="bash" OR like(process,"python%") OR process=="powershell","scripted input",    \
    process=="mongod", "KVStore") \
| eval process_class = case(    \
    process=="splunkd" AND (like(args,"-p %start%") OR like(args,"service")),"splunkd server",    \
    process=="splunkd" AND isnotnull(sid),"search",    \
    process=="splunkd" AND (like(args,"fsck%") OR like(args,"recover-metadata%") OR like(args,"cluster_thing")),"index service",    \
    process=="splunkd" AND args=="instrument-resource-usage", "scripted input",\
    (like(process,"python%") AND like(args,"%/appserver/mrsparkle/root.py%")) OR like(process,"splunkweb"),"Splunk Web",    \
    isnotnull(process_class), process_class)    \
| eval process_class = if(isnull(process_class),"other",process_class)
iseval = 0

[dmc_rename_introspection_fields]
# splunk search language doesn't like dot notation of json attributes, for now.
definition = eval process = 'data.process' \
| eval args = 'data.args' \
| eval sid = 'data.search_props.sid' \
| eval elapsed = 'data.elapsed' \
| eval mem_used = 'data.mem_used' \
| eval mem = 'data.mem' \
| eval pct_memory = 'data.pct_memory' \
| eval app = 'data.search_props.app' \
| eval type = 'data.search_props.type' \
| eval mode = 'data.search_props.mode' \
| eval user = 'data.search_props.user' \
| eval role = 'data.search_props.role'
iseval = 0

[dmc_resource_usage_by_processes_timechart(2)]
# this snippet is used for CPU usage chart and physical memory usage chart.
args = resource_type, function
definition = `dmc_set_bin` \
| stats latest($resource_type$) AS resource_usage_dedup latest(process_class) AS process_class by data.pid, _time \
| stats sum(resource_usage_dedup) AS resource_usage by _time, process_class \
| timechart minspan=10s $function$(resource_usage) AS "Resource Usage" by process_class
iseval = 0

[dmc_set_bin]
# force span equal to collection interval of resource_usage.log, to allow us deduplicate events in each time interval
definition = bin _time span=10s
iseval = 0

[dmc_set_bin_for_metrics_log]
# set bin span, because we use timechart a lot.
definition = bin _time minspan=30s
iseval = 0

[dmc_set_bin_for_disk_usage]
# force span equal to collection interval of disk_usage.log, to allow us deduplicate events in each time interval
definition = bin _time span=10min
iseval = 0

[dmc_set_bin_for_timechart]
# this is paired with dmc_timechart
definition = bin _time minspan=10s
iseval = 0

[dmc_set_bin_for_timechart_for_disk_usage]
# this is paired with dmc_timechart_for_disk_usage
definition = bin _time minspan=10min
iseval = 0

[dmc_timechart]
# this's a work around. In some cases timechart has too few columns, so we manually set many columns here.
definition = timechart minspan=10s
iseval = 0

[dmc_timechart_for_disk_usage]
# this's a work around. In some cases timechart has too few columns, so we manually set many columns here. This is specifically for disk usage charts.
definition = timechart minspan=10min
iseval = 0

[dmc_timechart_for_metrics_log]
# this's a work around. In some cases timechart has too few columns, so we manually set many columns here. This is specifically for metrics.log.
definition = timechart minspan=30s
iseval = 0

[time_modifier(1)]
# for scheduler assistant chart
args = time_modifier
definition = tostring(relative_time(time(), "$time_modifier$"))
iseval = 1

[dmc_get_distsearch_group_servers(1)]
args = group_name
definition = rest splunk_server=local /services/search/distributed/groups \
| fields title member \
| search title="dmc_group_$group_name$" \
| eval title=replace(title, "^dmc_group_", "") \
| eval servers=mvjoin(member,",") \
| fields title servers
iseval = 0

[dmc_pct_cpu_rangemap]
definition = rangemap field=resource_usage \
"0-100%"=0-100 \
"100-200%"=100.01-200 \
"200-300%"=200.01-300 \
"300-400%"=300.01-400 \
"400-500%"=400.01-500 \
"500-1000%"=500.01-1000 \
"1000-1500%"=1000.01-1500 \
"1500-2000%"=1500.01-2000 \
"2000-2500%"=2000.01-2500 \
"2500-3000%"=2500.01-3000 \
"3000%+"=3000.01-99999 \
"negative"=-99999--0.001 \
default="None"
iseval = 0

[dmc_pct_cpu_rangemap_and_timechart]
definition = `dmc_pct_cpu_rangemap` \
| `dmc_timechart` partial=f dc(host) AS host_count by range \
| fields _time, "3000%+" "2500-3000%" "2000-2500%" "1500-2000%" "1000-1500%" "500-1000%" "400-500%" "300-400%" "200-300%" "100-200%" "0-100%" "None"
iseval = 0

[dmc_mem_used_rangemap]
definition = rangemap field=resource_usage \
"0-1GB"=0-1024 \
"1-2GB"=1024-2048 \
"2-3GB"=2048-3072 \
"3-4GB"=3072-4096 \
"4-5GB"=4096-5120 \
"5-10GB"=5120-10240 \
"10-15GB"=10240-15360 \
"15-20GB"=15360-20480 \
"20-30GB"=20480-30720 \
"30GB+"=30720-999999 \
default="None" \
| eval resource_usage = round(resource_usage / 1024, 3)
iseval = 0

[dmc_mem_used_rangemap_and_timechart]
definition = `dmc_mem_used_rangemap` \
| `dmc_timechart` partial=f dc(host) AS host_count by range \
| fields _time, "30GB+" "20-30GB" "15-20GB" "5-10GB" "4-5GB" "3-4GB" "2-3GB" "1-2GB" "0-1GB" "None"
iseval = 0

[dmc_search_count_rangemap]
definition = rangemap field=search_count "0"=0-0 "1-5"=1-5 "6-10"=6-10 "11-15"=11-15 "16-20"=16-20 "21-30"=21-30 "31-40"=31-40 "41-50"=41-50 "51-75"=51-75 "76-100"=76-100 "101+"=101-99999
iseval = 0

[dmc_search_count_rangemap_and_timechart]
definition = `dmc_search_count_rangemap` \
| `dmc_timechart` dc(host) AS count_host by range \
| fields _time "101+" "76-100" "51-75" "41-50" "31-40" "21-30" "16-20" "11-15" "6-10" "1-5" "0"
iseval = 0

[dmc_indexing_rate_rangemap]
definition = rangemap field=kbps "0-50 KB/s"=0-50 "50-100 KB/s"=51-100 "100-500 KB/s"=101-500 "500 KB/s - 1 MB/s"=500-1024 "1-2.5 MB/s"=1025-2560 "2.5-5 MB/s"=2561-5120 "5-10 MB/s"=5121-10240 "10+ MB/s"=10241-999999 default="NULL"
iseval = 0

[dmc_indexing_rate_rangemap_and_timechart]
definition = `dmc_indexing_rate_rangemap` \
| timechart minspan=30s partial=f dc(host) AS instance_count by range \
| fields _time "10+ MB/s" "5-10 MB/s" "2.5-5 MB/s" "1-2.5 MB/s" "500 KB/s - 1 MB/s" "100-500 KB/s" "50-100 KB/s" "0-50 KB/s"
iseval = 0

[dmc_queue_fill_ratio_rangemap]
definition = rangemap field=fill_percentage "0-60%"=0-60 "60-80%"=60.01-80 "80-100%"=80.01-100
iseval = 0

[dmc_queue_fill_ratio_rangemap_and_timechart]
definition = `dmc_queue_fill_ratio_rangemap` \
| timechart partial=f dc(host) by range \
| fields _time "80-100%" "60-80%" "0-60%"
iseval = 0

[dmc_cpu_usage_rangemap]
definition = rangemap field=cpu_usage "0-60%"=0-60 "60-80%"=60.001-80 "80-100%"=80.001-100 "100%+"=100.001-999999 default=abnormal
iseval = 0

[dmc_cpu_usage_rangemap_and_timechart]
definition = `dmc_cpu_usage_rangemap` \
| `dmc_timechart` partial=f dc(server) as server_count by range \
| fields _time, "100%+", "80-100%", "60-80%", "0-60%"
iseval = 0

[dmc_memory_usage_rangemap]
definition = rangemap field=pct_mem_used "0-60%"=0-0.6 "60-80%"=0.6-0.8 "80-100%"=0.8-1 default=abnormal
iseval = 0

[dmc_memory_usage_rangemap_and_timechart]
definition = `dmc_memory_usage_rangemap` \
| `dmc_timechart` partial=f dc(server) as server_count by range \
| fields _time, "80-100%", "60-80%", "0-60%"
iseval = 0

[dmc_disk_usage_rangemap]
definition = rangemap field=pct_disk_usage "0-60%"=0-0.6 "60-80%"=0.6-0.8 "80-100%"=0.8-1 default=abnormal
iseval = 0

[dmc_disk_usage_rangemap_and_timechart]
definition = `dmc_disk_usage_rangemap` \
| `dmc_timechart_for_disk_usage` partial=f dc(server_mount_point) as server_mount_point_count by range \
| fields _time, "80-100%", "60-80%", "0-60%"
iseval = 0

[dmc_audit_get_searches(1)]:
args = host
definition = `dmc_set_index_audit` host=$host$ action=search sourcetype=audittrail search_id=* \
| eval user = if(user="n/a", null(), user)
iseval = 0

[dmc_drilldown_join_peers_by_peerURI]
# this macro is used to get the following fields: serverName, machine, CPU, RAM, version
# NOTE: host and host_fqdn are not available in splunk 6.1 or earlier
definition = lookup dmc_assets host OUTPUT peerURI, serverName, machine \
| eval peerURI = mvindex(peerURI, 0) \
| eval machine = mvindex(machine, 0) \
| eval serverName = mvindex(serverName, 0) \
| join type=outer peerURI \
 [| rest splunk_server=local /services/search/distributed/peers \
  | rename title as peerURI \
  | append  \
    [| rest splunk_server=local /services/server/info \
     | eval peerURI = "localhost" \
    ] \
  | eval ram = round(physicalMemoryMB / 1024, 2)." GB" \
  | fields peerURI, version, numberOfCores, ram \
 ] \
| eval Action = serverName \
| fields - _time
iseval = 0

[dmc_drilldown_search_activity_deployment_search_concurrency(4)]
args = group, span, searchFunction, metric
definition = `dmc_set_index_introspection` search_group=$group$ sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide) \
| `dmc_set_bin` \
| stats dc(data.search_props.sid) AS distinct_search_count values(Action) as Action by host, _time \
| bin _time span=$span$ \
| stats $searchFunction$(distinct_search_count) as search_count values(Action) as Action by host, _time \
| `dmc_search_count_rangemap` \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, search_count, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, search_count as "Count of Searches", range as "Count of Searches Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_search_activity_deployment_resource_usage(5)]
args = group, span, resourceType, resourceFunction, metric
definition = `dmc_set_index_introspection` search_group=$group$ sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide) \
| `dmc_set_bin` \
| stats latest(data.$resourceType$) AS resource_usage_dedup values(Action) as Action by _time, data.search_props.sid, data.pid, host \
| stats sum(resource_usage_dedup) AS sum_resource_usage values(Action) as Action by _time, host \
| bin _time span=$span$ \
| stats $resourceFunction$(sum_resource_usage) as resource_usage values(Action) as Action by _time, host \
| `dmc_$resourceType$_rangemap` \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, resource_usage, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, resource_usage as "$resourceType$ Usage", range as "$resourceType$ Usage Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_indexing_performance_deployment_indexing_rate(3)]
args = group, span, metric
definition = `dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=$group$ group=thruput \
| timechart span=$span$ partial=f limit=0 per_second(kb) AS kbps by host \
| untable _time host kbps \
| eval kbps = round(kbps,0) \
| `dmc_indexing_rate_rangemap` \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, kbps, range, numberOfCores, ram, version, Action\
| eval kbps = kbps." KB/s" \
| rename serverName as Instance, machine as Machine, kbps as "Indexing Rate", range as "Indexing Rate Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_indexing_performance_deployment_queue_fill_ratio(5)]
args = group, span, queueType, funcQueue, metric
definition = `dmc_set_index_internal` source=*metrics.log sourcetype=splunkd search_group=$group$ group=queue name=$queueType$ \
| bin _time span=$span$ \
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size) \
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size) \
| eval fill_perc=round((curr/max)*100,2) \
| bin _time minspan=30s \
| stats $funcQueue$(fill_perc) AS fill_percentage by host, _time \
| `dmc_queue_fill_ratio_rangemap` \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, fill_percentage, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, fill_percentage as "Fill Ratio (%)", range as "Fill Ratio Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_resource_usage_deployment_cpu_usage(4)]
args = group, span, countCPUFunc, metric
definition = `dmc_set_index_introspection` search_group=$group$ source=*resource_usage.log sourcetype=splunk_resource_usage component=Hostwide \
| bin _time span=$span$ \
| eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct' \
| `dmc_set_bin_for_timechart` \
| stats $countCPUFunc$(total_cpu_usage) as cpu_usage by host _time \
| `dmc_cpu_usage_rangemap` \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, cpu_usage, range, numberOfCores, ram, version, Action \
| eval cpu_usage = cpu_usage." %" \
| rename serverName as Instance, machine as Machine, cpu_usage as "CPU Usage", range as "CPU Usage Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_resource_usage_deployment_memory_usage(4)]
args = group, span, countMemFunc, metric
definition = `dmc_set_index_introspection` search_group=$group$ source=*resource_usage.log sourcetype=splunk_resource_usage component=Hostwide \
| bin _time span=$span$ \
| eval pct_mem_used = 'data.mem_used' / 'data.mem' \
| `dmc_set_bin_for_timechart` \
| stats $countMemFunc$(pct_mem_used) as pct_mem_used by host _time \
| `dmc_memory_usage_rangemap` \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, pct_mem_used, range, numberOfCores, ram, version, Action \
| eval pct_mem_used = round(pct_mem_used * 100, 2) \
| eval pct_mem_used = pct_mem_used." %" \
| rename serverName as Instance, machine as Machine, pct_mem_used as "Memory Usage", range as "Memory Usage Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_resource_usage_deployment_disk_usage(4)]
args = group, span, countDiskFunc, metric
definition = `dmc_set_index_introspection` search_group=$group$ sourcetype=splunk_disk_objects component=Partitions \
| bin _time span=$span$ \
| eval mount_point = 'data.mount_point' \
| eval free = if(isnotnull('data.available'), 'data.available', 'data.free') \
| eval pct_disk_usage = round(1 - free / 'data.capacity', 2) \
| `dmc_set_bin_for_timechart_for_disk_usage` \
| eval server_mount_point = host.":".mount_point \
| stats $countDiskFunc$(pct_disk_usage) as pct_disk_usage by server_mount_point _time \
| `dmc_disk_usage_rangemap` \
| where range="$metric$" \
| eval host = replace(server_mount_point, ":.*", "") \
| eval mount_point = replace(server_mount_point, ".*:", "") \
| eval pct_disk_usage = pct_disk_usage * 100 \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, mount_point, pct_disk_usage, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, mount_point as "Mount Point", pct_disk_usage as "Disk Usage (%)", range as "Disk Usage Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_kv_store_deployment_page_faults(3)]
args = span, countPageFaultsFunc, metric
definition = `dmc_set_index_introspection` search_group=dmc_group_kv_store component=kvstoreserverstats  \
| rename data.opcounters.command as c, data.opcounters.update as u, data.opcounters.queries as q, data.opcountes.deletes as d, data.opcounters.getmore as g, data.opcounters.inserts as i \
| eval commands=if(isNotNull('c'), 'c',0) \
| eval updates=if(isNotNull('u'), 'u', 0) \
| eval queries=if(isNotNull('q'), 'q', 0) \
| eval deletes=if(isNotNull('d'), 'd', 0) \
| eval getmores=if(isNotNull('g'), 'g', 0) \
| eval inserts=if(isNotNull('i'), 'i', 0) \
| eval totalops=commands+updates+queries+deletes+getmores+inserts \
| bin _time minspan=30s \
| stats latest(totalops) AS ops latest(data.extra_info.page_faults) AS pf by host _time \
| eval  percent=if(opsdiff==0, 0, round(abs(pf/ops), 2))   \
| stats $countPageFaultsFunc$(percent) as percent by host  \
| rangemap field=percent "0-0.7"=0-0.7 "0.7-1.3"=0.7001-1.3 "1.3+"=1.3001-999999 default=abnormal  \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, percent, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, percent as "Page Faults per Operation", range as "Page Fault Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_kv_store_deployment_lock_percentage(3)]
args = span, countLockFunc, metric
definition = `dmc_set_index_introspection` search_group=dmc_group_kv_store component=kvstoreserverstats \
| stats range(data.globalLock.lockTime) AS globalLock range(data.globalLock.totalTime) AS total by host  \
| eval percent=abs(globalLock*100/total)   \
| stats $countLockFunc$(percent) as percent_locked by host \
| rangemap field=percent_locked "0-30%"=0-30 "30-50%"=30-50 "50-100%"=50-1000 default=abnormal  \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, percent_locked, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, percent_locked as "Lock (%)", range as "Lock Percentage Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_kv_store_deployment_network]
definition =`dmc_set_index_introspection` component=kvstoreserverstats \
| bin _time span=1m \
| stats range(data.network.bytesIn) as indiff range(data.network.bytesOut) as outdiff range(data.network.numRequests) as Requests by host \
| eval "MB In"=indiff/1000000 \
| eval "MB Out"=outdiff/1000000 \
| `dmc_drilldown_join_peers_by_peerURI` \
| table serverName, machine, "MB In", "MB Out", Requests, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_kv_store_deployment_memory_ratio(2)]
args = span, metric
definition = `dmc_set_index_introspection` search_group=dmc_group_kv_store component=kvstoreserverstats \
| eval ratio='data.mem.virtual'/'data.mem.mappedWithJournal' \
| stats avg(ratio) AS myratio by host  \
| rangemap field=myratio "0-2x"=0-2 "2-3x"=2-3 ">3x"=3-10000 default=abnormal \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, myratio, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, myratio as "Virtual to Mapped Ratio", range as "Ratio Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_drilldown_kv_store_deployment_replication_lag(2)]
args  = span, metric
definition= `dmc_set_index_introspection` search_group=dmc_group_kv_store component=kvstorereplicasetstats  data.replSetStats.myState!=1 \
| bin _time span=1m \
| stats max(data.oplogInfo.end) AS local by _time host \
| join _time \
  [ search `dmc_set_index_introspection` search_group=dmc_group_kv_store component=kvstorereplicasetstats data.replSetStats.myState=1 \
  | bin _time span=1m \
  | stats max(data.oplogInfo.end) AS primary by _time] \
| eval difference=(primary-local)/1000 \
| stats avg(difference) AS Lag by host _time \
| rangemap field=Lag "0-10s"=0-10 "10-30s"=10-30 ">30s"=30-10000000 default="abnormal" \
| where range="$metric$" \
| rename host as Instance, range as "Lag Range" \
| table Instance Lag "Oplog Date" "Lag Range" State 
iseval = 0

[dmc_drilldown_kv_store_primary_oplog_window]
definition= `dmc_set_index_introspection` search_group=dmc_group_kv_store component=kvstorereplicasetstats data.replSetStats.myState=1 \
| eval Instance=host \
| bin _time span=1m \
| rename data.oplogInfo.start as start, data.oplogInfo.end as end \
| eval diff=(end-start)/36000000 \
| stats first(start) as "Start Date" first(end) as "End Date" first(diff) as "Window (hours)" by Instance 
iseval=0

[dmc_drilldown_kv_store_deployment_background_flush(2)]
args = span, metric
definition = `dmc_set_index_introspection`  search_group=dmc_group_kv_store component=kvstoreserverstats \
| stats range(data.backgroundFlushing.total_ms)  AS diff by  host \
| eval percent=abs(diff/600) \
| rangemap field=percent "0-10%"=0-10 "10-50%"=10-50 "50-100%"=50-100 default=abnormal \
| where range="$metric$" \
| `dmc_drilldown_join_peers_by_peerURI` \
| fields serverName, machine, percent, range, numberOfCores, ram, version, Action \
| rename serverName as Instance, machine as Machine, percent as "Background Flush (%)", range as "Background Flush Range", version as Version, numberOfCores as Cores, ram as RAM
iseval = 0

[dmc_instances_view_default_search(1)]
args = group
definition = inputlookup assets.csv \
| search search_group = $group$ \
| mvcombine search_group \
| join type=outer peerURI \
  [| rest splunk_server=local /services/search/distributed/peers \
   | rename title as peerURI ] \
| join type=outer peerURI \
  [| rest splunk_server=local /services/server/info \
   | eval peerURI = "localhost" \
   | eval status = "Up"] \
| join peerURI \
  [| `dmc_get_instance_roles` ] \
| eval status = if(status == "Up", status, "Unreachable") \
| eval Action = serverName \
| eval OS = os_name \
| eval ram = round(physicalMemoryMB / 1024, 2)." GB" \
| fields serverName, machine, role, OS, numberOfCores, ram, version, status, Action \
| rename serverName as Instance, machine as Machine, role as Role, numberOfCores as "Cores", ram as RAM, version as Version, status as Status
iseval = 0

[dmc_get_recently_triggered_alerts]
definition = rest splunk_server=local /servicesNS/admin/splunk_management_console/saved/searches \
|rename title as savedsearch_name \
| fields savedsearch_name, triggered_alert_count, description \
| where triggered_alert_count > 0 \
| join type=outer savedsearch_name[|rest splunk_server=local /servicesNS/admin/splunk_management_console/alerts/fired_alerts/- \
| cluster field=savedsearch_name] \
| join type=outer sid [rest splunk_server=local /servicesNS/nobody/splunk_management_console/search/jobs/] \
| rename savedsearch_name AS "Alert Name", triggered_alert_count AS "Trigger Count", published AS "Last time triggered", description as "Description" | fields "Alert Name" , "Trigger Count", "Last time triggered", "Description", sid
iseval = 0

[dmc_get_all_triggered_alerts(1)]
args = minute_offset
definition = rest splunk_server=local /servicesNS/admin/splunk_management_console/alerts/fired_alerts/- \
| rename savedsearch_name as title \
| join type=outer title [|rest splunk_server=local /servicesNS/admin/splunk_management_console/saved/searches ] \
| rename title as "Alert Name" \
| join type=outer sid [rest splunk_server=local /servicesNS/nobody/splunk_management_console/search/jobs/] \
| eval now=relative_time(now(),"-$minute_offset$m") \
| where trigger_time > now \
| rename trigger_time AS "_time", description as "Description", sid AS Instance \
| convert timeformat="%b. %d, %Y %l:%M %p" ctime(_time) AS "Time Triggered" \
| fields - _time \
| fields "Alert Name", Instance, "Time Triggered", "Description"
iseval = 0

[dmc_dmc_group_license_master]
definition = splunk_server_group=dmc_group_license_master
iseval = 0

[dmc_daily_license_usage_quota]
definition = rest `dmc_dmc_group_license_master` /services/licenser/pools \
| join type=outer stack_id splunk_server [rest `dmc_dmc_group_license_master` /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields splunk_server stack_id is_active] \
| search is_active=1 \
| fields splunk_server, stack_id, used_bytes \
| join type=outer stack_id splunk_server [rest `dmc_dmc_group_license_master` /services/licenser/stacks | eval stack_id=title | eval stack_quota=quota | fields splunk_server stack_id stack_quota] \
| stats sum(used_bytes) as used_bytes max(stack_quota) as stack_quota by splunk_server \
| eval usedGB=round(used_bytes/1024/1024/1024,3) \
| eval totalGB=round(stack_quota/1024/1024/1024,3) \
| eval percentage=round(usedGB / totalGB, 3)*100 \
| fields splunk_server, percentage, usedGB, totalGB
iseval = 0

[dmc_get_local_instance_asset]
definition = `dmc_get_local_instance_asset_computed_groups` \
| join type=outer peerURI [ \
| rest splunk_server=local /services/search/distributed/groups \
| fields title member \
| where isnotnull(mvfind(member, "localhost:localhost")) \
| eval peerURI="localhost" \
| rename title AS search_groups \
| fields peerURI search_groups \
| mvcombine delim=" " search_groups] \
| makemv delim=" " search_groups \
| eval search_groups = if(isnotnull(search_groups),search_groups,computed_search_groups) \
| fields - computed_search_groups \
| mvexpand search_groups
iseval = 0

[dmc_get_local_instance_asset_computed_groups]
definition = rest splunk_server=local /services/server/info \
| eval peerURI="localhost" \
| eval server_roles="indexer,search_head,license_master,kv_store" \
| makemv delim="," server_roles \
| mvexpand server_roles \
| eval computed_search_groups="dmc_group_".server_roles \
| stats first(peerURI) AS peerURI first(host) AS host first(host_fqdn) AS machine values(computed_search_groups) AS computed_search_groups by serverName 
